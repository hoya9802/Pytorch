


import torch
from torch import nn


x = torch.tensor([1.])
model = nn.Linear(1,1)   # 1 linear unit with input node 1, output node 1

# Initialize weight, bias of model as you create it
print(model.weight)
print(model.bias)

y = model(x)
print(y)
print('=='*30)
y = x @ model.weight + model.bias
print(y)


# fully connected layer
fc1 = nn.Linear(1,3)
fc2 = nn.Linear(3,1)

print(fc1.weight)
print(fc1.bias)
print(fc2.weight)
print(fc2.bias)
print('=='*30)
x = torch.tensor([1.])
x = fc1(x)
print(x)
x = fc2(x)
print(x)
print('=='*30)
x = torch.tensor([1.])
print((x@fc1.weight.T + fc1.bias)@fc2.weight.T + fc2.bias)





fc1 = nn.Linear(1,3)
fc2 = nn.Linear(3,1)

nn.Sequential(fc1, fc2) # connect each layers

x = torch.tensor([1.])
print(model(x))


model = nn.Sequential(nn.Linear(2,5),     # it means nn.Linear("channel", "channel")
              nn.Linear(5,10),
              nn.Linear(10,3))

x = torch.randn(2)
print(x)
print(model(x))

x = torch.randn(1,2)
print(x)
print(model(x))

print("==="*30)
x = torch.randn(5,2) # randn(number, channel)
print(x)
print(model(x))

print("==="*30)
x = torch.randn(2,3,1,4,5,2)
print(model(x).shape)        # Only the last number is "channel". Everything else is "number"


class Mymodel1(nn.Module):
    def __init__(self):
        super().__init__()

        self.fc1 = nn.Linear(2,5)
        self.fc2 = nn.Linear(5,10)
        self.fc3 = nn.Linear(10,3)
        self.sig = nn.Sigmoid()
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.sig(x)
        x = self.fc2(x)
        x = self.sig(x)
        x = self.fc3(x)
        x = self.sig(x)
        
        return x

model = Mymodel1()
x = torch.randn(5,2)
y = model(x)
print(y)


print(model)
print(model.fc1.weight)
print(model.fc2.bias)


class Mymodel2(nn.Module):
    def __init__(self):
        super().__init__()

        self.linear = nn.Sequential(nn.Linear(2,5),
                                    nn.Sigmoid(),
                                    nn.Linear(5,10),
                                    nn.Sigmoid(),
                                    nn.Linear(10,3),
                                    nn.Sigmoid())
    
    def forward(self, x):
        x = self.linear(x)
        
        return x

model2 = Mymodel2()
x = torch.randn(5,2)
y = model2(x)
print(y)


print(model2.linear[0].weight)
print(model2.linear[-2].bias)


list(model.parameters())


# you can get the number of parameters
num = sum([p.numel() for p in model.parameters() if p.requires_grad])
print(num)





import torch
from torch import nn

Fin = 5000
Fout = 1000
w = torch.zeros(141, Fin)
nn.init.kaiming_uniform_(w, mode="fan_in", nonlinearity='relu')
print(w.std())
print(torch.sqrt(torch.tensor(2/Fin)))

w = torch.zeros(Fout, 212)
nn.init.kaiming_uniform_(w, mode="fan_out", nonlinearity="relu")
print(w.std())
print(torch.sqrt(torch.tensor(2/Fout)))

# CNN?
N=32
C=64
H=6
W=10
w = torch.zeros(N,C,H,W)
nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')
print(w.std())
print(torch.sqrt(torch.tensor(2/(C*H*W))))



