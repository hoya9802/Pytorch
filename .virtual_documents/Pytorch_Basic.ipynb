


import torch


a = torch.tensor([1, 2, 3, 4])
print(a)
print(type(a))
print(a.dtype)
print(a.shape)
b = torch.tensor([1, 2, 3.1, 4])
print(b.dtype)


A = torch.tensor([[1, 2], [3, 4]])
# A = torch.tensor([[1, 2], [3, 4, 5]])
print(A)
print(A.shape)
print(A.ndim)
print(A.numel()) # if you want to get the same result of A.size of Numpy, you can use A.numel() instead!


print(torch.zeros(5))
print(torch.zeros_like(A))
print(torch.ones(5))
print(torch.zeros(3, 3))
print(torch.arange(3, 10, 2))
print(torch.arange(0, 1, 0.1))
print(torch.linspace(0, 1, 10)) # In Torch, endpoint in linspace is not available!


a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])
c = a + b
print(c)


A = torch.tensor([[1, 2, 3], [1, 2, 3]])
B = torch.tensor([[4, 5, 6], [1, 1, 1]])
C = A+B
D = A-B
print(C)
print(D)
print()
print(A*B)
print(A/B)
print(B**2)


A = torch.tensor([[1, 2], [3, 4]])
B = torch.tensor([[1, 2], [3, 4]])
print(A*B)
print(A@B)


a = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
print(a[0])
print(a[1])
print(a[-1])
print(a[1:4])
print(a[7])
print(a[7:])
print(a[:7])
print(a[:])


A = torch.tensor([[1, 2, 3],[4, 5, 6], [7, 8, 9]])
print(A[0])
print(A[-1])
print(A[1:])
print(A[:])
print(A[0][2])
print(A[0, 2])
B = [[1, 2, 3, 4], [5, 6, 7, 8]]
print(B)
print(B[0][2])
print(A[1,:])
print(A[1,0:3:2])
print(A[:,2])
print(A[:][2])


A = torch.tensor([[[0,1,2,3],[4,5,6,7],[8,9,10,11]],
                  [[12,13,14,15],[16,17,18,19],[20,21,22,23]]])
print(A)
print(A.shape)
print(A[0,1,2])

a = torch.tensor([[[1,2,3,4]]])
print(a.shape)


# boolean indexing
a = [1,2,3,4,5,3,3]
print(a==3) # False: it's because type of a is list and "3" is just str. list is not equal to str
A = torch.tensor([[1,2,3,4], [5,3,7,3]])
print(A == 3) # it works in Pytorch!
print(A[A==3])

A[A==3] = 100
print(A)

A = torch.tensor([[1,2],[3,4],[5,6],[7,8]])
B = [True, False, False, True]
print(A[B, :])

b = torch.tensor([1,2,3,4])
print(b[[True, True, False, False]]) # The version of inserting list into pytorch index of b


# tensor indexing
a = torch.tensor([1,2,3,4,5])
A = a[2]
print(A)
A = a[torch.tensor(2)]
print(A)
A = a[torch.tensor([2,3,4])]
print(A)
A = a[torch.tensor([[2,2,2],[3,3,3]])]
print(A)

a = torch.tensor([[1,2,3],[4,5,6]])
print(a[0])
A = a[torch.tensor([[0,1],[1,1]])]
print(A.shape)
print(A)


A = torch.tensor([[1,2],[3,4],[5,6],[7,8]])
print(A)
print(A.shape)

# 1. A[row, column]
print(A[0, 1])
# 2. A[ tensor(bool)] => bool list should be same with shape of A
print(A[torch.tensor([[False, True],[False, False],[False,True],[True, False]])])
print(A[A==2])
# 3. A[which index does True occur, which index does True occur]
print(A[[True, False, False, False], [False, True]])
# 4. A[tensor] => how do you stack the index of your choice
print(A[torch.tensor([1,1,2,2,2])])

print(A[torch.tensor([[0, 1], [2, 3]])])
print(A[torch.tensor([[0, 1], [2, 3]])].shape)


# Don't need ".random"
A = torch.randn(3, 3) # randn => random number of normal distribution
B = torch.rand(3, 3) # rand => random number(0 to 1) of uniform. Only Get positive real number
print(A)
print(B)
print(A[:,0])
print(A[A[:,0]<0, :])


A = torch.randn(3,3)
print(A)
print(torch.abs(A))
print(torch.sqrt(torch.abs(A)))
print(torch.exp(torch.tensor(1))) # e^1
print(torch.log(torch.abs(A)))
print(torch.log(torch.exp(torch.tensor(1)))) # log_e(e^1)
print(torch.log10(torch.tensor(10)))
print(torch.log2(torch.tensor(2)))
print(torch.round(A))
print(torch.round(A, decimals=2)) # up to the second decimal place
print(torch.floor(A)) # round up
print(torch.ceil(A)) # round down


# torch.pi is originally float but if you try to calculate with torch.tensor() then it would be change tensor
print(torch.sin(torch.tensor(torch.pi/6)))
print(torch.cos(torch.tensor(torch.pi/3)))
print(torch.tan(torch.tensor(torch.pi/4)))
print(torch.tanh(torch.tensor(-10)))

type(torch.tensor(1)/6)


torch.nan
print(torch.log(torch.tensor(-1)))
print(torch.isnan(torch.tensor([1,2,torch.nan,3,4]))) # you can check if there has non-number
print(torch.isinf(torch.tensor([1,2,3,4,torch.inf])))


A=torch.randn(3,4)
print(A)
print(torch.max(A))
print(torch.max(A,dim=0)) # this function change into one dimension tensor
print(torch.max(A,dim=1))
print(torch.max(A,dim=0, keepdims=True)) # this function show the result without changing demension
print(torch.max(A,dim=1, keepdims=True))

print(torch.min(A))
print(torch.min(A,dim=0))
print(torch.min(A,dim=1))
print(torch.argmax(A))
print(torch.argmax(A,dim=0)) # argmax => you can get the index of highest number in Tensor!
print(torch.argmax(A,dim=1))


A = torch.randn(6,1)
print(A)
a_sorted = torch.sort(A, dim=0)
print(a_sorted)
# you can only get indices of sorted tensor
print(a_sorted[1])

a = torch.randn(6,1)
print(a)
print(a.sort(dim=0))
print(a.sort(dim=0, descending=True))

print('#'*100)
print(torch.max(a))
print(a.max())
print(torch.abs(a))
print(a.abs())


A = torch.randn(3,4)
print(A)
print(torch.sum(A))
print(torch.sum(A, dim=1))
print(torch.sum(A, dim=1, keepdim=True))
print('=='*50)
print(torch.mean(A))
print(torch.mean(A, dim=1))
print(torch.mean(A, dim=1, keepdim=True))
print(torch.std(A))


A = torch.randint(1,5,size=(24,)) # Generates random integers from 1 to 4 with the specified size of (12,)
print(A)
print(A.shape)

B = A.reshape(2,3,4)
print(B)
print(B.ndim)
print(B.sum())
print(B.sum(dim=0, keepdim=True))
print(B.sum(dim=1, keepdim=True))
print(B.sum(dim=2, keepdim=True))


a = torch.tensor([1,2,3])
b = torch.tensor([2,2,1])
print(torch.sum(a*b))
a = a.reshape(3,1)
b = b.reshape(3,1)
print(a.transpose(1,0)@b)
print(a.permute(1,0)@b) # it would be more useful in more than 3 dimensions and beyond
print(a.T@b)  # it would be more useful in less than 2 dimestions
print(a.t()@b)

A = torch.randn(4,3,6)
print(A.permute(0,2,1).shape)
print(A.transpose(0,2).shape) # transpose() only can change 2 indecies


A = torch.arange(20)
print(A)
print(A.reshape(4,5).shape)
print(A.reshape(4,-1).shape)
print(A.reshape(2,5,-1).shape)
print(A.reshape(2,-1,5).shape)
print(A.reshape(1,-1).shape)  # row vector
print(A.reshape(-1,1).shape)  # column vector


x = torch.randn(2,3,4,5,6)
print(x[1,2,:,:,:].shape)  # x[1, 2, ...] is equaled to x[1, 2, :, :, :]
print(x[1,2,...].shape)
print(x[:,:,:,:,3].shape)
print(x[...,3].shape)
print(x[1,:,:,3,:].shape)
print(x[1,...,3,:].shape)


A = torch.ones((2,3,4))
B = torch.zeros((2,3,4))

C = torch.hstack([A,B]) # don't use this function
D = torch.vstack([A,B]) # don't use this function

E = torch.cat([A,B], dim=0)
F = torch.cat([A,B], dim=1)
G = torch.cat([A,B], dim=2)

print(A)
print(B)
print(C)
print(D)
print(E)
print(F)
print(G)


A = torch.randn(1,1,1,3,1,1,4,1)
print(A.shape)
print(A.squeeze().shape)


A = torch.randn(3,4)
print(A.unsqueeze(dim=0).shape)
print(A.unsqueeze(dim=1).shape)
print(A.unsqueeze(dim=2).shape)
print(A.reshape(1,3,4).shape)
print(A.reshape(3,1,4).shape)
print(A.reshape(3,4,1).shape)


A = torch.tensor([[1,2], [3,4]])
B = A              # It share A address. So if you change element of B tensor, then the element of tensor of A is changed as well!
C = B.clone()      # you can use this function not to share the tensor address that you try to copy!
B[0,0] = 100

print(B)
print(A)
print(C)





A = torch.randn(5,7)
B = torch.randn(7,10)
C = A@B
print(C.shape)

A = torch.randn(32,5,7)
B = torch.randn(32,7,10)
C = A@B                  # if it's 3 dimension, it is calculated by dimension-wise inner product
print(C.shape)


A = torch.randn(32,5,7)
B = torch.randn(7,10)

print((A@B).shape)
print(B.repeat(32,1,1).shape)
print((A@B.repeat(32,1,1)).shape)

a = torch.randn(3,4)
print(a.repeat(3,1,2,3).shape)


import numpy as np
a = np.array([1,2,3])
b = torch.tensor([1,2,3])
A = torch.tensor(a)
print(type(A))
B = b.numpy()
print(type(B))





x = torch.tensor([1.], requires_grad=True) # Must be float
print(x)


x = torch.tensor([1.])
print(x)
print(x.requires_grad)
x.requires_grad = True
print(x)
print(x.requires_grad)


x = torch.tensor([1.], requires_grad=True)
y = x**2
print(y)

y.backward()
print(x.grad)


x = torch.tensor([1.], requires_grad=True)
y = x**2
print(y)
y.retain_grad()       # If you want to get the grad of mid tensor such as y, you can use it

z = 3*y
print(z)

z.backward()
print(x.grad)         # It figures out this grad by using chain rule
print(y.grad)         # By using .retain_grad() function, now you can get the y grad


x=torch.tensor([1.], requires_grad=True)
a = x**2
b = a+1
print(b)
c=b**2
c.backward()
print(x.grad)


x = torch.tensor([1.], requires_grad=True)
y = torch.tensor([1.], requires_grad=True)
z = 2*x**2 + y**2
z.backward()
print(x.grad)
print(y.grad)


x = torch.tensor([1., 2., 3.], requires_grad=True)
y = torch.sum(x**2)             # that means x1**2 + x2**2 + x3**2
y.backward()
print(y)
print(x.grad)


x = torch.tensor([1.], requires_grad=True)
x.requires_grad = False

y = x**2
print(y)
# y.backward()     error!


x = torch.tensor([2.], requires_grad=True)
x = x.detach()
print(x)
y = x**2
print(y)


# detach and torch.no_grad
x = torch.tensor([1.], requires_grad=True)
# if you don't want to calculate the grad_fn temporary, you can use torch.no_grad()
# because you don't need to use Memerory when you test a model!
with torch.no_grad():
    y = x**2
    print(x.requires_grad)
    print(y)
print(x.requires_grad)
# y.backward() would be error at this line!!
y = x**2
print(y)       # now it works becuase the y = x**2 is out of the "with torch.no_grad()"" function


x = torch.tensor([-1.], requires_grad=True)
y = torch.abs(x)

print(y)
y.backward()
print(x.grad)



