import torch





# choose the dataset you want
# case 1
# N = 20
# random0 = torch.randn(int(N/2), 1)
# random5 = torch.randn(int(N/2), 1) + 5
# class1_data = torch.hstack([random0, random5])
# class2_data = torch.hstack([random5, random0])

# class1_label = torch.ones(int(N/2),1)
# class2_label = torch.zeros(int(N/2),1)
# x = torch.vstack([class1_data, class2_data])
# y = torch.vstack([class1_label, class2_label])

# case 2
N = 100
radius1=0.3+0.05*torch.randn(int(N/2),1)
radius2=1+0.01*torch.randn(int(N/2),1)
theta1=2*torch.pi*torch.rand(int(N/2),1)-torch.pi
theta2=2*torch.pi*torch.rand(int(N/2),1)-torch.pi
class1_data=torch.hstack([radius1*torch.cos(theta1), radius1*torch.sin(theta1)])
class2_data=torch.hstack([radius2*torch.cos(theta2), radius2*torch.sin(theta2)])
class1_label=torch.ones(int(N/2),1)
class2_label=torch.zeros(int(N/2),1)
x = torch.vstack([class1_data, class2_data])
y = torch.vstack([class1_label, class2_label])


import matplotlib.pyplot as plt
plt.plot(class1_data[:,0], class1_data[:,1], 'o')
plt.plot(class2_data[:,0], class2_data[:,1], 'ro')
plt.xlabel('x1')
plt.ylabel('y1')
plt.grid()





from torch import nn

class MLP(nn.Module):
    def __init__(self):
        super().__init__()

        
        # case 1. plain
        self.linear = nn.Sequential(nn.Linear(2, 100),
                                    nn.Sigmoid(),
                                    nn.Linear(100, 1),
                                    nn.Sigmoid())

        # case 3. very deep
        # self.linear = nn.Sequential(nn.Linear(2, 100),
        #                        nn.ReLU(),
        #                        nn.Sequential(*[i for _ in range(10) for i in [nn.Linear(100,100), nn.ReLu()]]),
        #                        nn.Linear(100, 1),
        #                        nn.Sigmoid())

    def forward(self, x):
        x = self.linear(x)
        return x


nn.Sequential(*[i for _ in range(5) for i in [nn.Linear(100, 100), nn.Sigmoid()]])


model = MLP()
print(model)





from torch import optim
import torch.nn.functional as F

LR = 1e-2 
EPOCH = 400 

# optimizer  = optim.SGD(model.parameters(), lr=LR) # optimizor for case 1
optimizer  = optim.Adam(model.parameters(), lr=LR)   # optimizor for case 2

loss_history = []

model.train() # change into train mode
for it in range(EPOCH):
    # inference
    y_hat = model(x)
    # loss
    loss = F.binary_cross_entropy(y_hat, y) # y_hat(prediction) should be in front of y(target) ==> (y_hat, y)
    # update
    optimizer.zero_grad() # Prevent stack of gradient
    loss.backward()       # BPP(Back Propagation)
    optimizer.step()      # Weight update
    # print loss
    loss_history += [loss.item()]
    print(f'Epoch: {it+1}, train loss: {round(loss.item(),3)}')
    print('-'*20)


x = torch.tensor([1.], requires_grad=True)
for _ in range(2):
    loss=x**2
    loss.backward()
    print(x.grad)    # if you use loss.backward() without The functions below, the result would be stacked by addition.  
    x.grad = None    # x.grad = 0 doesn't work


print(F.binary_cross_entropy(y_hat, y))
print(-torch.sum(torch.log(y_hat**y*(1-y_hat)**(1-y)))/N)


plt.plot(loss_history)
plt.xlabel('Epoch')
plt.ylabel('loss')





x1_test = torch.linspace(-10,10,30) # case 1
x2_test = torch.linspace(-10,10,30) # case 2
X1_test, X2_test = torch.meshgrid(x1_test, x2_test)
X_test = torch.cat([X1_test.unsqueeze(dim=2), X2_test.unsqueeze(dim=2)], dim=2)


model.eval()      # change into test mode 
with torch.no_grad():
    y_hat = model(X_test)
# 1. Train mode and Test mode in Dropout or BN have quite different operation! so you should use eval() function!
# 2. Calculate grad_fn <- it is unnecessary!

Y_hat = y_hat.squeeze()

plt.figure(figsize=[10, 9])  # figsize=[가로, 세로]
ax = plt.axes(projection='3d')
ax.view_init(elev=25,azim=-140)
ax.plot_surface(X1_test, X2_test, Y_hat.numpy(), cmap='viridis', alpha=0.2)
plt.plot(class1_data[:,0],class1_data[:,1],class1_label.squeeze(),'bo')
plt.plot(class2_data[:,0],class2_data[:,1],class2_label.squeeze(),'ro')
plt.xlabel('x1')
plt.ylabel('x2')



